{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is HGST_5TB\n",
      " Volume Serial Number is 4CF5-8CDE\n",
      "\n",
      " Directory of D:\\GitHub\\USC-Data-Bootcamp\\unit_13_project2_repo\\company-info\n",
      "\n",
      "06/27/2020  12:08 PM    <DIR>          .\n",
      "06/27/2020  12:08 PM    <DIR>          ..\n",
      "06/27/2020  10:40 AM    <DIR>          .ipynb_checkpoints\n",
      "06/27/2020  12:00 PM    <DIR>          _archived\n",
      "06/27/2020  12:08 PM            27,079 scrape_company_info.ipynb\n",
      "               1 File(s)         27,079 bytes\n",
      "               4 Dir(s)  3,957,297,328,128 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../csv/company_info.csv')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "csv_path = pathlib.Path(f'../csv/company_info.csv')\n",
    "csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-af14da4fb3e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcompany_info_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 8, saw 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "company_info_df = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CITIES = {'New York', 'Chicago', 'San Francisco', 'Austin', 'Seattle',\n",
    "                  'Los Angeles', 'Philadelphia', 'Atlanta', 'Dallas',\n",
    "                  'Pittsburgh', 'Portland', 'Phoenix', 'Denver', 'Houston',\n",
    "                  'Miami'}\n",
    "\n",
    "YOUR_CITIES = {'Boston', 'Washington DC', 'St Louis', 'San Diego',\n",
    "               'San Antonio', 'Columbus', 'Sacramento', 'Charlotte', 'Memphis',\n",
    "               'Detroit', 'Nashville', 'Jacksonville', 'Indianapolis',\n",
    "               'Fort Worth', 'Charlotte', 'El Paso', 'Oklahoma City',\n",
    "               'Las Vegas', 'Louisville', 'Milwaukee', 'Albuquerque', 'Tucson',\n",
    "               'Kansas City', 'Mesa', 'Colorado Springs', 'Raleigh', 'Omaha',\n",
    "               'Virginia Beach', 'Minneapolis', 'New Orleans', 'Tampa',\n",
    "               'San Jose', 'Baltimore', 'Fresno', 'Oakland', 'Tulsa', 'Madison',\n",
    "               'Arlington', 'Wichita', 'Cleveland', 'Aurora', 'Honolulu',\n",
    "               'Orlando', 'Anchorage', 'Des Moines', 'Salt Lake City',\n",
    "               'Lexington', 'Cincinnati', 'Newark', 'Durham', 'Buffalo',\n",
    "               'Baton Rouge', 'Richmond', 'Boise', 'Birmingham', 'Little Rock',\n",
    "               'Grand Rapids', 'Worcester', 'Providence', 'Sioux Falls',\n",
    "               'Jackson', 'Hartford', 'Bridgeport', 'Jersey City', 'Charleston',\n",
    "               'Billings', 'Fargo', 'Augusta'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_location(result):\n",
    "    \"\"\"extract job location\"\"\"\n",
    "    try:\n",
    "        location = result.find('span', class_='location').get_text().strip()\n",
    "        return location\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company(result):\n",
    "    \"\"\"extract the name of the company\"\"\"\n",
    "    try:\n",
    "        company = result.find('span', class_='company').get_text().strip()\n",
    "        return company\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(result):\n",
    "    \"\"\"extract the job title\"\"\"\n",
    "    try:\n",
    "        title = result.find('a', attrs={'data-tn-element': \"jobTitle\"}).get('title')\n",
    "        return title\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_star(result):\n",
    "    \"\"\"extract a number (width) that is proportional to the number of stars\n",
    "    shown for the company\"\"\"\n",
    "    try:\n",
    "        # the 'style' attribute dictates how many stars are filled with color\n",
    "        star = result.find('span', class_='ratingsContent').get_text()\n",
    "        # extract only the number\n",
    "        star = star.replace('\"\"', '').replace('\\n', '')\n",
    "        return star\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary(result):\n",
    "    \"\"\"extract the salary\"\"\"\n",
    "    try:\n",
    "        salary = result.find('span', class_='salaryText').get_text().strip()\n",
    "        return salary\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.indeed.com/jobs\"\n",
    "params = {'q': 'data scientist', 'radius': '100'}\n",
    "# params = {'radius': '100'}\n",
    "max_results = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Miami&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Portland&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Austin&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Dallas&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Phoenix&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Denver&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=San+Francisco&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=New+York&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Atlanta&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Philadelphia&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Pittsburgh&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Los+Angeles&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Chicago&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Seattle&start=0\n",
      "------------------------------------------------------------------------\n",
      "https://www.indeed.com/jobs?q=data+scientist&radius=100&l=Houston&start=0\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for city in DEFAULT_CITIES | YOUR_CITIES:\n",
    "for city in DEFAULT_CITIES:\n",
    "    for start in range(0, max_results, 10):\n",
    "        url_params = params.copy()\n",
    "        url_params.update({'l': city, 'start': start})\n",
    "        scraped_data = {'location': [],\n",
    "                    'company': [],\n",
    "                    'title': [],\n",
    "#                     'salary' :[],\n",
    "                    'star': []}\n",
    "        response = requests.get(url, params=url_params)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        results = soup.find_all('div', class_='result')\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "        print(response.url)\n",
    "        print('------------------------------------------------------------------------')\n",
    "        \n",
    "        for result in results:\n",
    "            scraped_data['location'].append(extract_location(result))\n",
    "            scraped_data['company'].append(extract_company(result))\n",
    "            scraped_data['title'].append(extract_title(result))\n",
    "#             scraped_data['salary'].append(extract_salary(result))\n",
    "            scraped_data['star'].append(extract_star(result))\n",
    "        \n",
    "        company_info_df = pd.DataFrame(scraped_data)\n",
    "        \n",
    "        company_info_df.to_csv('../csv/company.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>title</th>\n",
       "      <th>salary</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>HP</td>\n",
       "      <td>Data Scientist (Sales and Marketing)</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Pilot Flying J</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>Perceptronics Solutions, Inc</td>\n",
       "      <td>AI/Machine Learning Research Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Pilot Flying J</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>National Oilwell Varco</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spring, TX 77389</td>\n",
       "      <td>HP</td>\n",
       "      <td>Data Scientist (Sales and Marketing)</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>vedainfo</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Houston, TX 77084</td>\n",
       "      <td>Aramco Services Company</td>\n",
       "      <td>Data Scientist (501652)</td>\n",
       "      <td>None</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>SelectMinds</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Houston, TX 77019 (Neartown - Montrose area)</td>\n",
       "      <td>BDO</td>\n",
       "      <td>Software Developer Intern (Data Analytics Team...</td>\n",
       "      <td>None</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Houston, TX 77077</td>\n",
       "      <td>Techwave Consulting Inc.</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Aspen Technology</td>\n",
       "      <td>SR Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>BHP</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>None</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>Genius Sports Media</td>\n",
       "      <td>Sports Statistician</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        location  \\\n",
       "0                                           None   \n",
       "1                                           None   \n",
       "2                                           None   \n",
       "3                                    Houston, TX   \n",
       "4                                    Houston, TX   \n",
       "5                               Spring, TX 77389   \n",
       "6                                    Houston, TX   \n",
       "7                              Houston, TX 77084   \n",
       "8                                    Houston, TX   \n",
       "9   Houston, TX 77019 (Neartown - Montrose area)   \n",
       "10                             Houston, TX 77077   \n",
       "11                                   Houston, TX   \n",
       "12                                   Houston, TX   \n",
       "13                                          None   \n",
       "\n",
       "                         company  \\\n",
       "0                             HP   \n",
       "1                 Pilot Flying J   \n",
       "2   Perceptronics Solutions, Inc   \n",
       "3                 Pilot Flying J   \n",
       "4         National Oilwell Varco   \n",
       "5                             HP   \n",
       "6                       vedainfo   \n",
       "7        Aramco Services Company   \n",
       "8                    SelectMinds   \n",
       "9                            BDO   \n",
       "10      Techwave Consulting Inc.   \n",
       "11              Aspen Technology   \n",
       "12                           BHP   \n",
       "13           Genius Sports Media   \n",
       "\n",
       "                                                title salary  star  \n",
       "0                Data Scientist (Sales and Marketing)   None   4.0  \n",
       "1                                      Data Scientist   None   3.3  \n",
       "2              AI/Machine Learning Research Scientist   None  None  \n",
       "3                                      Data Scientist   None   3.3  \n",
       "4                                      Data Scientist   None   3.9  \n",
       "5                Data Scientist (Sales and Marketing)   None   4.0  \n",
       "6                                      Data Scientist   None  None  \n",
       "7                             Data Scientist (501652)   None   3.9  \n",
       "8                                      Data Scientist   None  None  \n",
       "9   Software Developer Intern (Data Analytics Team...   None   3.7  \n",
       "10                                     Data Scientist   None  None  \n",
       "11                                  SR Data Scientist   None   3.0  \n",
       "12                           Principal Data Scientist   None   4.1  \n",
       "13                                Sports Statistician   None  None  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
