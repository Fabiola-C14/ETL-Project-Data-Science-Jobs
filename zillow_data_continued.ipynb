{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list= [\"Detroit\", \"Livonia\", \"Newport Beach\", \"Orlando\", \"Bloomfield Hills\",\n",
    "            \"Bellevue\", \"Lansing\", \"Ypsilanti\", \"Bloomington\", \"Austin\", \"Alpharetta\",\n",
    "            \"Dearborn\", \"Chicago\", \"Alvin\", \"Oakland\", \"Redmond\", \"Denver\" \"Washington DC\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Detroit data\n",
    "DT_url = 'https://www.zillow.com/homes/Detroit,-MI_rb/'\n",
    "browser.visit(DT_url)\n",
    "DT_html = browser.html\n",
    "DT_soup = BeautifulSoup(DT_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    DTresults= DT_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in DTresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    DT_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "DTnew = DT_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "DT_listing_df[\"Street\"]= DTnew[0] \n",
    "# making separate city column from new data frame \n",
    "DT_listing_df[\"City\"]= DTnew[1] \n",
    "# making separate city column from new data frame \n",
    "DT_listing_df[\"State\"]=DTnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "DT_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "DT_listing_df.to_csv(\"DT_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Livonia data\n",
    "LV_url = 'https://www.zillow.com/homes/Livonia,-MI_rb/'\n",
    "browser.visit(LV_url)\n",
    "LV_html = browser.html\n",
    "LV_soup = BeautifulSoup(LV_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    LVresults= LV_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in LVresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    LV_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "LVnew = LV_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "LV_listing_df[\"Street\"]= LVnew[0] \n",
    "# making separate city column from new data frame \n",
    "LV_listing_df[\"City\"]= LVnew[1] \n",
    "# making separate city column from new data frame \n",
    "LV_listing_df[\"State\"]=LVnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "LV_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "LV_listing_df.to_csv(\"LV_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow New Port Beach data\n",
    "NB_url = 'https://www.zillow.com/homes/Newport-Beach,-CA_rb/'\n",
    "browser.visit(NB_url)\n",
    "NB_html = browser.html\n",
    "NB_soup = BeautifulSoup(NB_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    NBresults= NB_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in NBresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    NB_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "NBnew = NB_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "NB_listing_df[\"Street\"]= NBnew[0] \n",
    "# making separate city column from new data frame \n",
    "NB_listing_df[\"City\"]= NBnew[1] \n",
    "# making separate city column from new data frame \n",
    "NB_listing_df[\"State\"]=NBnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "NB_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "NB_listing_df.to_csv(\"NB_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Orlando data\n",
    "OR_url = 'https://www.zillow.com/homes/Orlando,-FL_rb/'\n",
    "browser.visit(OR_url)\n",
    "OR_html = browser.html\n",
    "OR_soup = BeautifulSoup(OR_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    ORresults= OR_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in ORresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    OR_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "ORnew = OR_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "OR_listing_df[\"Street\"]= ORnew[0] \n",
    "# making separate city column from new data frame \n",
    "OR_listing_df[\"City\"]= ORnew[1] \n",
    "# making separate city column from new data frame \n",
    "OR_listing_df[\"State\"]=ORnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "OR_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "OR_listing_df.to_csv(\"OR_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Bloomfield Hills data\n",
    "BH_url = 'https://www.zillow.com/homes/Bloomfield-Hills,-MI_rb/'\n",
    "browser.visit(BH_url)\n",
    "BH_html = browser.html\n",
    "BH_soup = BeautifulSoup(BH_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    BHresults= BH_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in BHresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    BH_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "BHnew = BH_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "BH_listing_df[\"Street\"]= BHnew[0] \n",
    "# making separate city column from new data frame \n",
    "BH_listing_df[\"City\"]= BHnew[1] \n",
    "# making separate city column from new data frame \n",
    "BH_listing_df[\"State\"]=BHnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "BH_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "BH_listing_df.to_csv(\"BH_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Bellevue data\n",
    "BL_url = 'https://www.zillow.com/homes/Belleville,-Washington,-WV_rb/'\n",
    "browser.visit(BL_url)\n",
    "BL_html = browser.html\n",
    "BL_soup = BeautifulSoup(BL_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    BLresults= BL_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in BLresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    BL_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "BLnew = BL_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "BL_listing_df[\"Street\"]= BLnew[0] \n",
    "# making separate city column from new data frame \n",
    "BL_listing_df[\"City\"]= BLnew[1] \n",
    "# making separate city column from new data frame \n",
    "BL_listing_df[\"State\"]=BLnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "BL_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "BL_listing_df.to_csv(\"BL_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Lansing data\n",
    "LS_url = 'https://www.zillow.com/homes/Lansing,-MI_rb/'\n",
    "browser.visit(LS_url)\n",
    "LS_html = browser.html\n",
    "LS_soup = BeautifulSoup(LS_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    LSresults= LS_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in LSresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    LS_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "LSnew = LS_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "LS_listing_df[\"Street\"]= LSnew[0] \n",
    "# making separate city column from new data frame \n",
    "LS_listing_df[\"City\"]= LSnew[1] \n",
    "# making separate city column from new data frame \n",
    "LS_listing_df[\"State\"]=LSnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "LS_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "LS_listing_df.to_csv(\"LS_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Ypsilanti data\n",
    "YP_url = 'https://www.zillow.com/homes/Ypsilanti,-MI_rb/'\n",
    "browser.visit(YP_url)\n",
    "YP_html = browser.html\n",
    "YP_soup = BeautifulSoup(YP_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    YPresults= YP_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in YPresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    YP_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "YPnew = YP_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "YP_listing_df[\"Street\"]= YPnew[0] \n",
    "# making separate city column from new data frame \n",
    "YP_listing_df[\"City\"]= YPnew[1] \n",
    "# making separate city column from new data frame \n",
    "YP_listing_df[\"State\"]=YPnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "YP_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "YP_listing_df.to_csv(\"YP_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Bloomington data\n",
    "BT_url = 'https://www.zillow.com/homes/Bloomington,-IN_rb/'\n",
    "browser.visit(BT_url)\n",
    "BT_html = browser.html\n",
    "BT_soup = BeautifulSoup(BT_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    BTresults= BT_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in BTresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    BT_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "BTnew = BT_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "BT_listing_df[\"Street\"]= BTnew[0] \n",
    "# making separate city column from new data frame \n",
    "BT_listing_df[\"City\"]= BTnew[1] \n",
    "# making separate city column from new data frame \n",
    "BT_listing_df[\"State\"]=BTnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "BT_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "BT_listing_df.to_csv(\"BT_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Austin data\n",
    "AT_url = 'https://www.zillow.com/homes/Austin,-TX_rb/'\n",
    "browser.visit(AT_url)\n",
    "AT_html = browser.html\n",
    "AT_soup = BeautifulSoup(AT_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    ATresults= AT_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in ATresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    AT_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "ATnew = AT_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "AT_listing_df[\"Street\"]= ATnew[0] \n",
    "# making separate city column from new data frame \n",
    "AT_listing_df[\"City\"]= ATnew[1] \n",
    "# making separate city column from new data frame \n",
    "AT_listing_df[\"State\"]=ATnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "AT_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "AT_listing_df.to_csv(\"AT_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Alpharetta data\n",
    "AP_url = 'https://www.zillow.com/homes/Alpharetta,-GA_rb/'\n",
    "browser.visit(AP_url)\n",
    "AP_html = browser.html\n",
    "AP_soup = BeautifulSoup(AP_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    APresults= AP_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in APresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    AP_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "APnew = AP_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "AP_listing_df[\"Street\"]= APnew[0] \n",
    "# making separate city column from new data frame \n",
    "AP_listing_df[\"City\"]= APnew[1] \n",
    "# making separate city column from new data frame \n",
    "AP_listing_df[\"State\"]=APnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "AP_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "AP_listing_df.to_csv(\"AP_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Dearborn data\n",
    "DB_url = 'https://www.zillow.com/homes/Dearborn,-MI_rb/'\n",
    "browser.visit(DB_url)\n",
    "DB_html = browser.html\n",
    "DB_soup = BeautifulSoup(DB_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    DBresults= DB_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in DBresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    DB_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "DBnew = DB_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "DB_listing_df[\"Street\"]= DBnew[0] \n",
    "# making separate city column from new data frame \n",
    "DB_listing_df[\"City\"]= DBnew[1] \n",
    "# making separate city column from new data frame \n",
    "DB_listing_df[\"State\"]= DBnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "DB_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "DB_listing_df.to_csv(\"DB_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Chicago data\n",
    "CH_url = 'https://www.zillow.com/homes/Livonia,-MI_rb/'\n",
    "browser.visit(CH_url)\n",
    "CH_html = browser.html\n",
    "CH_soup = BeautifulSoup(CH_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    CHresults= CH_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in CHresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    CH_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "CHnew = CH_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "CH_listing_df[\"Street\"]= CHnew[0] \n",
    "# making separate city column from new data frame \n",
    "CH_listing_df[\"City\"]= CHnew[1] \n",
    "# making separate city column from new data frame \n",
    "CH_listing_df[\"State\"]=CHnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "CH_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "CH_listing_df.to_csv(\"CH_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Alvin data\n",
    "AV_url = 'https://www.zillow.com/homes/Alvin,-TX_rb/'\n",
    "browser.visit(AV_url)\n",
    "AV_html = browser.html\n",
    "AV_soup = BeautifulSoup(AV_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    AVresults= AV_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in AVresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    AV_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "AVnew = AV_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "AV_listing_df[\"Street\"]= AVnew[0] \n",
    "# making separate city column from new data frame \n",
    "AV_listing_df[\"City\"]= AVnew[1] \n",
    "# making separate city column from new data frame \n",
    "AV_listing_df[\"State\"]=AVnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "AV_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "AV_listing_df.to_csv(\"AV_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Oakland data\n",
    "OL_url = 'https://www.zillow.com/homes/Oakland,-CA_rb/'\n",
    "browser.visit(OL_url)\n",
    "OL_html = browser.html\n",
    "OL_soup = BeautifulSoup(OL_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    OLresults= OL_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in OLresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    OL_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "OLnew = OL_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "OL_listing_df[\"Street\"]= OLnew[0] \n",
    "# making separate city column from new data frame \n",
    "OL_listing_df[\"City\"]= OLnew[1] \n",
    "# making separate city column from new data frame \n",
    "OL_listing_df[\"State\"]=OLnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "OL_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "OL_listing_df.to_csv(\"OL_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Redmond data\n",
    "RM_url = 'https://www.zillow.com/homes/Redmond,-WA_rb/'\n",
    "browser.visit(RM_url)\n",
    "RM_html = browser.html\n",
    "RM_soup = BeautifulSoup(RM_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    RMresults= RM_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in RMresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    RM_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "RMnew = RM_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "RM_listing_df[\"Street\"]= RMnew[0] \n",
    "# making separate city column from new data frame \n",
    "RM_listing_df[\"City\"]= RMnew[1] \n",
    "# making separate city column from new data frame \n",
    "RM_listing_df[\"State\"]=RMnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "RM_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "RM_listing_df.to_csv(\"RM_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow Denver data\n",
    "DV_url = 'https://www.zillow.com/homes/Denver,-CO_rb/'\n",
    "browser.visit(DV_url)\n",
    "DV_html = browser.html\n",
    "DV_soup = BeautifulSoup(DV_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    DVresults= DV_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in DVresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    DV_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "DVnew = DV_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "DV_listing_df[\"Street\"]= DVnew[0] \n",
    "# making separate city column from new data frame \n",
    "DV_listing_df[\"City\"]= DVnew[1] \n",
    "# making separate city column from new data frame \n",
    "DV_listing_df[\"State\"]=DVnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "DV_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "DV_listing_df.to_csv(\"DV_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Zillow DC data\n",
    "DC_url = 'https://www.zillow.com/homes/Washington,-DC_rb/'\n",
    "browser.visit(DC_url)\n",
    "DC_html = browser.html\n",
    "DC_soup = BeautifulSoup(DC_html, 'html5lib')\n",
    "\n",
    "for x in range(1,4):\n",
    "\n",
    "    DCresults= DC_soup.find_all('div', class_='list-card-info')\n",
    "\n",
    "    address= []\n",
    "    asking_price=[]\n",
    "    for result in DCresults: \n",
    "   \n",
    "        location = result.find('address', class_='list-card-addr').text\n",
    "        price = result.find('div', class_='list-card-price').text\n",
    "\n",
    "        address.append(location)\n",
    "        asking_price.append(price)\n",
    "    \n",
    "    browser.links.find_by_partial_text('Next')\n",
    "    \n",
    "    #Create dataframe\n",
    "    DC_listing_df=pd.DataFrame ({\"Address\":address, \"Asking Price\": asking_price})\n",
    "\n",
    "\n",
    "# new data frame with split value columns \n",
    "DCnew = DC_listing_df[\"Address\"].str.split(\",\", n = 2, expand = True) \n",
    "  \n",
    "# making separate street column from new data frame \n",
    "DC_listing_df[\"Street\"]= DCnew[0] \n",
    "# making separate city column from new data frame \n",
    "DC_listing_df[\"City\"]= DCnew[1] \n",
    "# making separate city column from new data frame \n",
    "DC_listing_df[\"State\"]=DCnew[2] \n",
    "  \n",
    "# Dropping old Name columns \n",
    "DC_listing_df.drop(columns =[\"Address\"], inplace = True) \n",
    "\n",
    "#Create CVS extract\n",
    "DC_listing_df.to_csv(\"DC_data.csv\",encoding=\"utf-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all extracts into a single file\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
